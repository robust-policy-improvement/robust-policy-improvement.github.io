<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Blending Imitation and Reinforcement Learning for Robust Policy Improvement">
  <meta name="keywords" content="Reinforcement Learning, Imitation Learning, RL, Deep Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Blending Imitation and Reinforcement Learning for Robust Policy Improvement</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css"> -->
  <!-- <link rel="stylesheet" href="./static/css/bulma-slider.min.css"> -->
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script> -->
  <!-- <script src="./static/js/bulma-slider.min.js"></script> -->
  <!-- <script src="./static/js/index.js"></script> -->
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Blending Imitation and Reinforcement Learning for Robust Policy Improvement</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Xuefeng Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://takuma.yoneda.xyz">Takuma Yoneda</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.anl.gov/profile/rick-l-stevens">Rick L. Stevens</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://home.ttic.edu/~mwalter/">Matthew R. Walter</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://yuxinchen.org/">Yuxin Chen</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Chicago,</span>
            <span class="author-block"><sup>2</sup>TTI-Chicago</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=eJ0dzPJq1F"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.01737"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="javascript:;"
                   class="external-link button is-normal is-rounded is-light">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                   <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                   <i class="far fa-images"></i>
                   </span>
                   <span>Data</span>
                   </a>
                   </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src='./static/images/main-fig.png' id='teaser'/>
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
           <source src="./static/videos/teaser.mp4"
           type="video/mp4">
           </video> -->
      <!-- <h2 class="subtitle has-text-centered">
           Subtitle for the main figure: Please fill in here
           </h2> -->
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains.
          </p>
          <p>
            Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed.
            To address the demand for robust policy improvement in real-world scenarios, we introduce a novel algorithm, Robust Policy Improvement (RPI),
            which actively interleaves between IL and RL based on an online estimate of their performance.
          </p>
          <p>
            RPI draws on the strengths of IL, using oracle queries to facilitate exploration---an aspect that is notably challenging in sparse-reward RL---particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to
            RL, effectively treating the learned policy as an improved oracle.
            This algorithm is capable of learning from and improving upon a diverse set of
            black-box oracles.
          </p>
          <p>
            Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the oracles or learn from its own value function
            when the learner's performance surpasses that of the oracles in a specific state.
            Empirical evaluations and theoretical analysis validate that RPI excels in comparison to existing state-of-the-art methodologies, demonstrating superior performance across various benchmark domains.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
         <div class="column is-four-fifths">
         <h2 class="title is-3">Video</h2>
         <div class="publication-video">
         <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
         frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
         </div>
         </div>
         </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code> @inproceedings{liu2024blending,
      title={Blending Imitation and Reinforcement Learning for Robust Policy Improvement},
      author={Liu, Xuefeng and Yoneda, Takuma and Stevens, Rick and Walter, Matthew and Chen, Yuxin},
      booktitle={The Twelfth International Conference on Learning Representations},
      year={2024}
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
         <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
         <i class="fas fa-file-pdf"></i>
         </a>
         <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
         <i class="fab fa-github"></i>
         </a>
         </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
               This website is licensed under a <a rel="license"
               href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
               Commons Attribution-ShareAlike 4.0 International License</a>.
               </p> -->
          <p>
            The template of this website is borrowed from <a href="https://nerfies.github.io/">nerfies</a> website.
            The original template can be found <a href="https://github.com/nerfies/nerfies.github.io">here</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
